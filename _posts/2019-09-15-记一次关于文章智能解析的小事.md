---
layout:     post                    
title:      记一次关于文章智能解析的小事              
subtitle:   关于网页智能解析的应用与实现
date:       2019-09-15              
author:     financialfly                     
header-img: img/post-bg-os-metro.jpg    
catalog: true                       
tags:                               
    - python
    - webparse
---

前有好一段时间，我收到了来自`scrapyhub`团队的一封邮件，信上说他们在测试一款新产品，期待我的参与云云，这款产品叫做`AutoExtract-Beta`，一款关于自动解析的产品，基于机器学习技术，支持新闻和商品详情页的自动解析，当然由于还处于测试阶段，准确性有待提高。
 ![](https://github.com/financialfly/pics/blob/master/autoextract/640.png?raw=true)
我饶有兴致的试了一下，果然有待提高，随手提交了一下意见便没有再理会了，毕竟他们只暴露了`api`，没有多余的可以学习和观赏的东西。
话说今天你对我爱答不理，明天我让你高攀不起。当我收到来自公司的关于大量资讯信息的紧急任务时，我第一时间就想到了上面那位，不巧`AutoExtract-Beta`已经改名了，简单的讲就是把`Beta`去掉了，开始收费了。
![](https://github.com/financialfly/pics/blob/master/autoextract/6400.png?raw=true)
不过钱并不是问题，问题是…但不管怎样我还是决定试一下再说，于是我又充满期待的试了一下，效果并不好，因为业务上的缘故，我所要解析的页面很多情况下内容只有一句话，再给一个超链接，个别情况下，这句话比标题还短，导致解析的结果往往不尽人意。
我知道错误是不可避免的，但花钱买错误是不可取的，于是我开始了`Google`之旅。
![](https://github.com/financialfly/pics/blob/master/autoextract/640%20\(1\).png?raw=true)
![](https://github.com/financialfly/pics/blob/master/autoextract/640%20\(2\).png?raw=true) 
![](https://github.com/financialfly/pics/blob/master/autoextract/640%20\(5\).png?raw=true)
在经过大量的点击、阅读、尝试之后，我整理了现如今我面临几个重大的难题：
1.  太复杂的论文看不懂；
2.  看得懂不好用；
3.  好用又看的懂的贵；
4.  距离任务大限又近了一天。

当然，这些问题在一个敢于抗争，善于学习的人眼里，那都不叫事。而真正的事就是：
晚饭到底吃什么？

…

晚上吃完饭后，我找了一个很活跃的技术讨论群，并在群里面问了一个如何做智能解析问题，大家伙的回答各有千秋，我很感动，最终，我决定使用`readability`试一试，熟练搜索进入官网之后，我发现：
![](https://github.com/financialfly/pics/blob/master/autoextract/640%20\(4\).png?raw=true)     

在搜索`mercury`之前，我决定再找找`readability`的相关信息，想不到还有`python`版本，我当然是乐于使用了，当然结果跟预期还是有些出入。
于是我找到了`mercury`，坦白的说，对于我目前的业务，这个好用多了，比之前用`newspaper`，`readability`的`python`版本还有其它等等的库准确率要高不少。不过它是用`node.js`做的，而且似乎并不支持直接输入`html`来获取解析结果?总之，要结合我目前使用的爬虫还需要写一点代码，鉴于它也支持命令行，干脆就用命令行来吧，测试代码大概就是这样：
```py
# testmercury.py

import json
import html
import re
import subprocess

from functools import wraps

from lzz.settings import DEFAULT_REQUEST_HEADERS


URL = 'http://rsj.yinchuan.gov.cn/rsdt/tzgg/201908/t20190821_1678147.htm'


def check_args(func):

    @wraps(func)
    def check_args(*args, **kwargs):
        if args[0] is None:
            return
        return func(*args, **kwargs)

    return check_args


@check_args
def process_title(title):
    title = re.split(r'[-_]', title)[0]
    return title


@check_args
def process_addtime(addtime):
    return addtime[:10]


@check_args
def process_content(content):
    return html.unescape(content)


def test(url):
    mercury_commands = ['mercury-parser', url]
    for k, v in DEFAULT_REQUEST_HEADERS.items():
        mercury_commands.append(
            f'--headers.{k}={v}'
        )

    out_bytes = subprocess.check_output(mercury_commands, shell=True)
    result = out_bytes.decode('utf-8')
    result = json.loads(result)
    title = process_title(result['title'])
    addtime = process_addtime(result['date_published'])
    content = process_content(result['content'])
    print(title)
    print(addtime)
    print(content)


if __name__ == '__main__':
    test(URL)

```

很显然，最后我还是放弃了。为什么？因为慢的要死。

就在万念俱灰的时刻，我发现了一片名为[Readability内容分析算法，和它的那些多语言实现]( http://get.ftqq.com/130.get)的文章，文章中说到`readability`的大致算法实现就是遍历标签并对标签和文字做加权或降权分组，最后根据分值，重新拼接内容。实现了自动解析。

嗯，看起来也不算很难，试一下吧，先简单粗暴的来一个，由于项目是用`scrapy`开发的，就跟着`scrapy`的特性来了:
```py
# parsetool.py

import re


maybe_content_tags = ('div', 'td', 'table',)

filter_tags = ('script', 'style', 'h1', 'a', 'br',)


def iter_maybe_content_tags(selector):
    """返回可能为内容的标签下的所有子标签
    除去过滤的标签
    """
    query = ' | '.join(f'//body//{x}[@*]//*' for x in maybe_content_tags)
    for s in selector.xpath(query):
        tag = s.root.tag
        if tag in filter_tags:
            continue
        yield s


def extract_text_from_selector(s):
    """从selector对象中获取标签下的文字"""
    return ''.join(
        re.sub(r'\s', '', ls.get() or '') for ls in s.xpath('./text()')
    )


def parse_content_by_inner_selector(inner_selector):
    """根据当前标签往上找，找到可能为内容的标签后返回"""
    selector_list = inner_selector.xpath('./..')
    if not selector_list:
        return inner_selector.get()
    selector = selector_list[0]
    if selector.root.tag in maybe_content_tags:
        return selector.get()
    return parse_content_by_inner_selector(selector)


def guss_content_by_tag(response):
    """猜内容
    通过对标签打分 根据分数高的标签获取内容
    """
    # selector with priority
    sp = [response, -10]
    for s in iter_maybe_content_tags(response):
        text = extract_text_from_selector(s)
        if not text:
            continue
        priority = text_length = len(text) * 0.5
        if text_length > 200:
            priority += 10
        if s.xpath('./../script'):
            priority -= 20
        for i in s.xpath('.//img'):
            if i.xpath('./@src').get('').endswith(('jpg', 'png')):
                priority += 10
        if priority > sp[1]:
            sp[0] = s
            sp[1] = priority
    if sp[0] is not response:
        content = parse_content_by_inner_selector(sp[0])
        return content
```
好了，先看看效果吧，看完就失望了，这玩意也不见得好到哪里去，关键是它有时候不仅不能匹配到正确的内容，反而还匹配了一个错误的、糟糕的内容给我。

毕竟是自己写的，哭着也要改一下才行。最后，经过反复的修改和测试，并加入了更多的限制以防止其错误的输出(我宁愿接收一个`None`，也不愿接收一个错误的答案)，至于只有附件的内容，我特地加了一个附件识别的方法，当遇到附件时优先级就蹭蹭的往上涨。总算可以用了，当然，谨慎起见，我只是将它作为备用，以在特定规则失效的时候用它来顶替。其实它也运行的很好，没有犯过什么大错，接下来，还要做一些工作，比如删除那些常见的分享插件之类的非内容元素。

至于发布时间、作者以及标题等，解析方法相对简单就不再赘述了，网上的工具就很够用。最后要说的则是文章列表页的解析，结合自身项目内容分析发现，文章列表所在的a标签，其附近往往都有发布时间、评论数、阅读数等等固定形式的标签。虽然难以准确的找到文章的位置，但通过这些标签来寻找文章的位置也是一个讨巧的办法。以常见的发布时间为例，剔除其它干扰元素，它们的标签结构大概如下：
```html
<li>
    <a href="...">...</a>
    <span>2019-09-10</span>
</li>

或：

<li>
    <a href="...">...</a>
    <div>
        <span>2019-09-10</span>
    </div>
</li>
```
所以根据固定的时间格式找到发布时间所在标签，然后可以根据如下`xpath`规则定位到文章所在标签：
```
./..//a or ./../..//a
```
据此就可以开始撸代码了，如下：
```py
# parsetool.py


def isalnum(s):
    """
    str 对象提供的isalnum方法遇到中文时会输出错误的答案
    所以重写了一个
    """
    try:
        return s.encode('ascii').isalnum()
    except UnicodeEncodeError:
        return False


def extract_article_from_a_selector(selector, title_first=True):
    """从a标签中解析文章信息
    
    @title_first
        当匹配到文章a标签后
        是否优先将它的title属性(如果有)作为标题返回
    """
    link = selector.xpath('./@href').get()
    if title_first:
        title = selector.xpath('./@title').get()
        if not title or title.startswith(('标题', '文章标题')):
            title = extract_text_from_selector(selector)
    else:
        title = extract_text_from_selector(selector)
    return link, title


def parse_article_by_related_tag(response, text_pattern, title_first=True):
    """根据相邻标签文字解析文章
    @text_pattern
        文章发布日期或其它相邻标签文字的匹配模板
        type: re.Pattern
    """
    selectors = iter_maybe_article_selectors(response, text_pattern)
    return _iter_article(response, selectors, title_first)


# 该正则表达式会匹配到页码或其它纯数字文本
# 用isalnum方法筛选
PUBDATE_TEXT_PATTERN = re.compile(r'^[【（\(\[]?'  # 前阔号
                                  r'(?:\d{2,4}[-/\.年])?'  # 年份及后接字符
                                  r'\d{1,2}' # 月份及后接字符
                                  r'(?:[-/\.月]'  
                                  r'\d{1,2}?日?)?'  # 日期
                                  r'(?:\d{1,2}:\d{2}(?::\d{2})?)?'  # 时分秒
                                  r'[\]\)）】]?'  # 后阔号
                                  r'$')
def iter_maybe_article_selectors(response, text_pattern=None):
    """
    某些情况下该规则会匹配到很多a标签（有的网站会写很多）
    一般来说真正有用的（是文章所在标签的）会包含href和文字
    但在个别情况下文章所在a标签的text确实为空，但有title属性
    """
    if text_pattern is None:
        text_pattern = PUBDATE_TEXT_PATTERN

    for s in response.xpath('//*[text()!=""]'):
        text = extract_text_from_selector(s)
        if not text or isalnum(text) or not text_pattern.match(text):
            continue
        alist = s.xpath('./..//a') or s.xpath('./../..//a')
        for a in alist:
            yield a


def _iter_article(response, selectors, title_first):
    """解析文章列表"""
    seen = {response.url, }
    for sel in selectors:
        link, title = extract_article_from_a_selector(sel, title_first)
        if not link \
                or not title \
                or title.isspace() \
                or link in seen:
            continue
        seen.add(link)
        yield link, title
```
*注意以上文章列表解析规则只适用于结构较为简单的页面，搜狐、新浪等可以考虑从阅读量、评论量等入手。*

当然，以上种种，简单的、勉强算智能的解析方法也许并不会活得太久，毕竟人工智能的崛起速度是惊人的，空的时候，也许会专研一下关于机器学习抽取网页信息方面的东西，随缘再更新吧。
   
最后欢迎关注我的微信公众号：
![](https://github.com/financialfly/pics/blob/master/wechat.jpg?raw=true)