---
layout:     post                    
title:      scrapy代理中间件源码和自定义代理下载中间件              
subtitle:   如何在异步爬虫中更好的使用代理
date:       2019-05-14              
author:     financialfly                     
header-img: img/post-bg-os-metro.jpg    
catalog: true                       
tags:                               
    - python
    - scrapy
---


### scrapy代理中间件源码解析
```py
# from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware

# 源码：
class HttpProxyMiddleware(object):

    def __init__(self, auth_encoding='latin-1'):
        self.auth_encoding = auth_encoding
        # 初始化代理
        self.proxies = {}
        # getproxies -> from urllib.request import getproxies 即从os.environ环境变量中获取，只要遵循_proxy结尾的变量即可，具体可参见该方法源码
        for type_, url in getproxies().items():
            self.proxies[type_] = self._get_proxy(url, type_)

    @classmethod
    def from_crawler(cls, crawler):
        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):
            raise NotConfigured
        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')
        return cls(auth_encoding)

    def _basic_auth_header(self, username, password):
        user_pass = to_bytes(
            '%s:%s' % (unquote(username), unquote(password)),
            encoding=self.auth_encoding)
        return base64.b64encode(user_pass)

    def _get_proxy(self, url, orig_type):
        # url = 1.1.1.2
        # orig_type = 'http'
        # 如果有密码则为： {'http': 'http://user:password@192.168.11.11:9999/'}
        proxy_type, user, password, hostport = _parse_proxy(url)
        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))

        if user:
            creds = self._basic_auth_header(user, password)
        else:
            creds = None
        # 返回一个元组，如果有用户名和密码则为经过加密后的内容 -> creds 和地址 -> proxy_url
        # 如果没有用户名和密码则creds为None
        return creds, proxy_url

    def process_request(self, request, spider):
        # ignore if proxy is already set
        if 'proxy' in request.meta:
            if request.meta['proxy'] is None:
                return
            # extract credentials if present
            creds, proxy_url = self._get_proxy(request.meta['proxy'], '')
            request.meta['proxy'] = proxy_url
            if creds and not request.headers.get('Proxy-Authorization'):
                request.headers['Proxy-Authorization'] = b'Basic ' + creds
            return
        elif not self.proxies:
            return

        parsed = urlparse_cached(request)
        scheme = parsed.scheme

        # 'no_proxy' is only supported by http schemes
        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):
            return

        if scheme in self.proxies:
            self._set_proxy(request, scheme)

    def _set_proxy(self, request, scheme):
        creds, proxy = self.proxies[scheme]
        request.meta['proxy'] = proxy
        if creds:
            request.headers['Proxy-Authorization'] = b'Basic ' + creds
```

所以设置代理有三种方式：

#### 1. 在环境变量中添加
需要在代理中间件启动前加入到环境变量中，如在爬虫的`start_requests`方法中添加：
```py
# ...
	def start_requests(self):
		import os
		os.environ['HTTPS_PROXY'] = '192.11.2.32'
		# ...
```
*注：该方法只能在当前进程中有效，因为`os.environ`返回的是当前进程的环境变量，设置也只对当前进程有效*

#### 2. 通过`meta`传参
在请求时通过定义`meta`中的`proxy`值即可，这种方式的优先级更高

#### 3. 自定义代理下载中间件
主要注意在重写`middleware`时对获取代理的过程加锁，否则会造成多个代理被同时获取，导致代理被浪费。          

##### 方式一：直接获取并赋值
示例代码：
```python
import requests

from scrapy import signals
from twisted.internet.defer import DeferredLock # 异步锁，防止多次更换ip

import logging
logger = logging.getLogger('proxy')

from twisted.internet.error import TimeoutError, DNSLookupError, \
    ConnectionRefusedError, ConnectionDone, ConnectError, \
    ConnectionLost, TCPTimedOutError

class ProxyDownloadMiddleWare(object):

    ALL_EXCEPTIONS = (TimeoutError, TimeoutError, DNSLookupError,
                           ConnectionRefusedError, ConnectionDone, ConnectError,
                           ConnectionLost, TCPTimedOutError)

    def __init__(self):
        self.proxy = Proxy() # 代理对象
        self.lock = DeferredLock() # 锁对象

    def process_request(self, request, spider):
        ' 开始时检测是否加入代理，没有则加入 '
        if not request.meta.get('proxy'):
            request.meta['proxy'] = self.proxy.proxy

    def process_response(self, request, response, spider):
        ' 检测代理是否被封，可根据实际情况检测，如有的出现验证码的网页. '
        if response.status != 200:
            if not self.proxy.isbanned:
                self.proxy.isbanned = True
            spider.logger.debug('代理被加入黑名单了')
            self.update_proxy()
            return request
        return response

    # def process_exception(self, request, exception, spider):
    #    ' 代理出现超时等错误时，切换代理 '
    #     if isinstance(exception, self.ALL_EXCEPTIONS):
    #         print('\n代理超时，正在切换代理重试....\n')
    #         self.update_proxy()
    #     return request

    def update_proxy(self):
        ' 切换代理，切换时注意加锁，否则会造成多个进程同时取代理. '
        self.lock.acquire()
        if not self.proxy.proxy or self.proxy.isexpired or self.proxy.isbanned:
            self.proxy.update()
            logger.debug('成功切换代理：', self.proxy.proxy)
        self.lock.release()

class Proxy(object):
    '''代理对象'''

    def __init__(self, proxy_type='https'):
        self.proxy = None 
        self.isbanned = False
        self._type = proxy_type
        self.url = 'http://127.0.0.1:5000/get'

    def update(self):
        '''更新代理'''
        proxy = requests.post(self.url, data={'type': self.proxy_type})
        # ip = ...
        # port = ...
        self.proxy = '%s://%s:%s' % (self._type, ip, port)
        if self.proxy.isbanned:
            self.proxy.isbanned = False

    @property
    def isexpired(self):
        # do something to check the proxy is expired
        # return True if .. else False
        pass

    def set_type(self, proxy_type):
        '''
        设置代理类型
        如果请求的是使用https协议的网站，则类型为https，或者http则为http
        '''
        self._type = proxy_type
```
注意：上面的代码仍有一定的逻辑问题未处理，会导致代理复用率不高(线程重复获取或获取频率过高)。上述代码参考自：
```py
from twisted.internet.defer import DeferLock

class ProxyModel(object):

    def __init__(self, proxy_data):
        self.ip = proxy_data['ip']
        self.port = proxy_data['port']
        self.expire_time = proxy_data['expire_time']
        self.proxy = 'https://{}:{}'.format(self.ip, self.port)
        self.blacked = False

    @property
    def is_expiring(self):
        # do something..
        return True

class ProxyDownloadMiddleware(object):

    def __init__(self):
        self.current_proxy = None
        self.proxy_url = 'http://127.0.0.1:8080'
        self.lock = DeferLock()

    def process_request(self, request, spider):
        if 'proxy' not in request.meta or self.current_proxy.is_expiring:
            self.update_proxy()
        request.meta['proxy'] = self.current_proxy.proxy

    def process_response(self, request, response, spider):
        if response.status != 200:
            if not self.current_proxy.blacked:            
                self.current_proxy.blacked = True
            self.update_proxy()
            return request
        return response

    def update_proxy(self):
        self.lock.acquire()
        if not self.current_proxy or self.current_proxy.is_expiring or self.current_proxy.blacked:
            proxy_data = request.get(self.proxy_url)
            proxy_model = ProxyModel(proxy_data)
            self.current_proxy = proxy_model
        self.lock.release()
```

##### 方式二：动态维护一个少量代理列表
由于scrapy异步的特性，获取代理的速度也是相对较快。如果一个代理被`ban`了，有可能会导致同时获取了好几条新的代理。对此可以维护一个动态的代理列表，每次请求前随机从中获取一个作为代理，如果代理被`ban`了，则从列表中移除该代理，而每次移除时判断该列表剩余代理数是否低于指定阈值，如果低了则获取新的。这样做有利于代理的复用，不过也容易导致在爬虫最后列表中未用的代理会被浪费掉，但设置好阈值也不会浪费太多。实例代码：
```py
import random
from twisted.internet.defer import DeferredLock 

class ProxyDownloaderMiddleware(object):
    '''
    代理中间件
    '''
    ALL_EXCEPTIONS = (TimeoutError, TimeoutError, DNSLookupError,
                      ConnectionRefusedError, ConnectionDone, ConnectError,
                      ConnectionLost, TCPTimedOutError)
    status_codes = [520, 403]  # 被ban的状态码
    proxy_type = 'https'

    def __init__(self):
        self.proxy = Proxy(self.proxy_type)
        self.lock = DeferredLock()
        self.proxies = list()

    def process_request(self, request, spider):
        if self.proxies:
            request.meta['proxy'] = random.choice(self.proxies)

    def process_response(self, request, response, spider):
        if (not response.status in self.status_codes and
                not self.is_banned(request, response, spider)):
            return response
        current_proxy = request.meta.get('proxy')
        if current_proxy:
            logger.info('This proxy is banned: {}'.format(current_proxy))
            # 多个线程同时进行会导致错误，所以先进行判断
            if current_proxy in self.proxies:
                self.proxies.remove(current_proxy)
        self.keep_proxies()
        return request

    def process_exception(self, request, exception, spider):
        if not isinstance(exception, self.ALL_EXCEPTIONS):
            return
        self.keep_proxies()
        return request

    def keep_proxies(self):
        self.lock.acquire()
        if len(self.proxies) < 5:
            proxy = self.proxy.get()
            self.proxies.append(proxy)
            logger.info('Got a new proxy: {}'.format(proxy))
        self.lock.release()

    def is_banned(self, request, response, spider):
        '''
        检测是否被ban
        如果是则返回True，否返回False或None
        '''
        return False
```